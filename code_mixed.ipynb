{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1/2EaRfpx2E5T9kEon0oL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4c64ba4dd4834af9a57d1c6a08fd9137": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Sentence:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_165e029635f145ae978e1598c8d12c03",
            "placeholder": "Type a code-mixed sentence...",
            "style": "IPY_MODEL_2632692bb57f4142a9fd9eedcbde9e0f",
            "value": "hello yela unnav"
          }
        },
        "165e029635f145ae978e1598c8d12c03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "80%"
          }
        },
        "2632692bb57f4142a9fd9eedcbde9e0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1449a45667a24a71980e4b3564ba7458": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Analyze",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_e6159be2aa8245c5988c54f9eab0b801",
            "style": "IPY_MODEL_31fb56662fa849cf8ae5f2fd25197eff",
            "tooltip": ""
          }
        },
        "e6159be2aa8245c5988c54f9eab0b801": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31fb56662fa849cf8ae5f2fd25197eff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "0445227a11f044f8bf6e4eee6697fd4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Sentence:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_adc395ed5c8d4ee5ae5eec7e8d5791ce",
            "placeholder": "Type a code-mixed sentence...",
            "style": "IPY_MODEL_9dd4d1372b7b4f7992447b251637b2ca",
            "value": "hello yela unnav"
          }
        },
        "adc395ed5c8d4ee5ae5eec7e8d5791ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "80%"
          }
        },
        "9dd4d1372b7b4f7992447b251637b2ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2881e7b6ef5461b9d83aa44a3e9a342": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Analyze",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_34614a79812447aea9ecc9ade7db65fd",
            "style": "IPY_MODEL_73abf9c539f64b879086464a42135460",
            "tooltip": ""
          }
        },
        "34614a79812447aea9ecc9ade7db65fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73abf9c539f64b879086464a42135460": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "041ddb67a088416783fa9556b474e1b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Sentence:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_b8040622e53942d1ab5b582695c136d3",
            "placeholder": "Type a code-mixed sentence...",
            "style": "IPY_MODEL_b5dee9c671d4462dbed8ce4e2bb82b66",
            "value": "hello yela unnav"
          }
        },
        "b8040622e53942d1ab5b582695c136d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "80%"
          }
        },
        "b5dee9c671d4462dbed8ce4e2bb82b66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17439cddce6e4a23a619b6991348406f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Analyze",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_6c56a930c1d94830b5c2d01f9e840b1f",
            "style": "IPY_MODEL_b96f7d052def4d8e9c9ecbc59f3807a1",
            "tooltip": ""
          }
        },
        "6c56a930c1d94830b5c2d01f9e840b1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b96f7d052def4d8e9c9ecbc59f3807a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohith232/4.1-semester-b.tech-/blob/main/code_mixed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMipGEdq_YC9",
        "outputId": "0d64f588-4ba5-4ffe-d8c3-c1ca4e2a84e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=3373c223a4e946b38eeb7a3b37edfd868519dfffa12007d9bfbdb5e808bb8639\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ],
      "source": [
        "pip install pandas openpyxl scikit-learn langdetect\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from langdetect import detect\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# === Step 1: Load the dataset ===\n",
        "dataset_path = \"/content/code_mixed_sentences11.xlsx\"  # <-- Change this path\n",
        "df = pd.read_excel(dataset_path)\n",
        "\n",
        "# === Step 2: Tokenization & Language Detection ===\n",
        "def detect_languages(sentence):\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    lang_tags = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            lang = detect(token)\n",
        "        except:\n",
        "            lang = \"unknown\"\n",
        "        lang_tags.append(lang)\n",
        "    return tokens, lang_tags\n",
        "\n",
        "# === Step 3: Feature Extraction ===\n",
        "def extract_features(sentence):\n",
        "    tokens, lang_tags = detect_languages(sentence)\n",
        "    total = len(tokens)\n",
        "    en_count = lang_tags.count('en')\n",
        "    other_count = total - en_count\n",
        "    switch_count = sum(1 for i in range(1, len(lang_tags)) if lang_tags[i] != lang_tags[i-1])\n",
        "    return [total, en_count, other_count, switch_count, en_count/total if total else 0]\n",
        "\n",
        "# Extract features from code-mixed sentences\n",
        "df['features'] = df['CodeMixedSentence'].apply(extract_features)\n",
        "X_cm = np.array(df['features'].tolist())\n",
        "y_cm = np.ones(len(df))  # Label: 1 = code-mixed\n",
        "\n",
        "# === Step 4: Create synthetic non-code-mixed sentences ===\n",
        "synthetic_english = [\n",
        "    \"I went for a morning walk today.\",\n",
        "    \"The meeting is very important.\",\n",
        "    \"Would you like to have some coffee?\",\n",
        "    \"Let's watch a movie after dinner.\",\n",
        "    \"The teacher's explanation in class was great.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_telugu = [\n",
        "    \"Nenu subhodhayam walk ki vellanu.\",\n",
        "    \"E roju meeting chala mukhyam.\",\n",
        "    \"Meeru coffee teeyara?\",\n",
        "    \"Dinner tarvata cinema chuddam.\",\n",
        "    \"Class lo teacher explanation bagundi.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_sentences = synthetic_english + synthetic_telugu\n",
        "y_synth = [0] * len(synthetic_sentences)  # Label: 0 = not code-mixed\n",
        "X_synth = np.array([extract_features(sent) for sent in synthetic_sentences])\n",
        "\n",
        "# === Step 5: Combine datasets ===\n",
        "X = np.vstack([X_cm, X_synth])\n",
        "y = np.concatenate([y_cm, y_synth])\n",
        "\n",
        "# === Step 6: Train-Test Split & Train Model ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# === Step 7: Evaluation ===\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# === (Optional) Save the trained model ===\n",
        "joblib.dump(clf, \"code_mixed_detector_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "n4v_k3Ux_oU8",
        "outputId": "c407d318-348f-4a11-f09f-cc02b0a7f06e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected string or bytes-like object, got 'float'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-31f47324b40d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Extract features from code-mixed sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CodeMixedSentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mX_cm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0my_cm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Label: 1 = code-mixed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-31f47324b40d>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# === Step 3: Feature Extraction ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_languages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0men_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang_tags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-31f47324b40d>\u001b[0m in \u001b[0;36mdetect_languages\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# === Step 2: Tokenization & Language Detection ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetect_languages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\w+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mlang_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/re/__init__.py\u001b[0m in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     Empty matches are included in the result.\"\"\"\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object, got 'float'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from langdetect import detect\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# === Step 1: Load your uploaded Excel dataset ===\n",
        "df = pd.read_excel(\"code_mixed_sentences11.xlsx\")  # File from your upload\n",
        "print(\"Loaded dataset:\", df.shape)\n",
        "\n",
        "# === Step 2: Language Detection Helper ===\n",
        "def detect_languages(sentence):\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    lang_tags = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            lang = detect(token)\n",
        "        except:\n",
        "            lang = \"unknown\"\n",
        "        lang_tags.append(lang)\n",
        "    return tokens, lang_tags\n",
        "\n",
        "# === Step 3: Feature Extraction Function ===\n",
        "def extract_features(sentence):\n",
        "    tokens, lang_tags = detect_languages(sentence)\n",
        "    total = len(tokens)\n",
        "    en_count = lang_tags.count('en')\n",
        "    other_count = total - en_count\n",
        "    switch_count = sum(1 for i in range(1, len(lang_tags)) if lang_tags[i] != lang_tags[i-1])\n",
        "    return [total, en_count, other_count, switch_count, en_count/total if total else 0]\n",
        "\n",
        "# Apply feature extraction\n",
        "df['features'] = df['CodeMixedSentence'].apply(extract_features)\n",
        "X_cm = np.array(df['features'].tolist())\n",
        "y_cm = np.ones(len(df))  # Label 1 for code-mixed\n",
        "\n",
        "# === Step 4: Create synthetic non-code-mixed samples (label 0) ===\n",
        "synthetic_english = [\n",
        "    \"This is a nice day.\",\n",
        "    \"I am going to school.\",\n",
        "    \"We had dinner together.\",\n",
        "    \"Let's play football.\",\n",
        "    \"She reads every day.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_telugu = [\n",
        "    \"Nenu school ki veltunna.\",\n",
        "    \"Aame rojuki oka book chadutundi.\",\n",
        "    \"Manam kalisi dinner chesamu.\",\n",
        "    \"Football aadudham.\",\n",
        "    \"Idi chala bagundi.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_sentences = synthetic_english + synthetic_telugu\n",
        "y_synth = [0] * len(synthetic_sentences)\n",
        "X_synth = np.array([extract_features(sent) for sent in synthetic_sentences])\n",
        "\n",
        "# === Step 5: Combine datasets ===\n",
        "X = np.vstack([X_cm, X_synth])\n",
        "y = np.concatenate([y_cm, y_synth])\n",
        "\n",
        "# === Step 6: Train the Classifier ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# === Step 7: Evaluate the Model ===\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# === Optional: Save the model ===\n",
        "joblib.dump(clf, \"code_mixed_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "xMXuxHBvAnx3",
        "outputId": "7ec2e059-fd90-44d1-a372-199b81032ed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langdetect'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b21163b98f01>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangdetect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdetect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langdetect'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import get_ipython\n",
        "\n",
        "get_ipython().system('pip install langdetect')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxe4lnYH950L",
        "outputId": "72cca32e-32f5-46ac-c287-70f1d167ef03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/981.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m972.8/981.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=e5940a81b4899036b8e1b851b2dad653d9821591f1e168bd898dcac093075a89\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zMTsJjBr-CV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from langdetect import detect\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# === Step 1: Load your uploaded Excel dataset ===\n",
        "df = pd.read_excel(/code_mixed_sentences11.xlsx)  # File from your upload\n",
        "print(\"Loaded dataset:\", df.shape)\n",
        "\n",
        "# === Step 2: Language Detection Helper ===\n",
        "def detect_languages(sentence):\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    lang_tags = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            lang = detect(token)\n",
        "        except:\n",
        "            lang = \"unknown\"\n",
        "        lang_tags.append(lang)\n",
        "    return tokens, lang_tags\n",
        "\n",
        "# === Step 3: Feature Extraction Function ===\n",
        "def extract_features(sentence):\n",
        "    tokens, lang_tags = detect_languages(sentence)\n",
        "    total = len(tokens)\n",
        "    en_count = lang_tags.count('en')\n",
        "    other_count = total - en_count\n",
        "    switch_count = sum(1 for i in range(1, len(lang_tags)) if lang_tags[i] != lang_tags[i-1])\n",
        "    return [total, en_count, other_count, switch_count, en_count/total if total else 0]\n",
        "\n",
        "# Apply feature extraction\n",
        "df['features'] = df['CodeMixedSentence'].apply(extract_features)\n",
        "X_cm = np.array(df['features'].tolist())\n",
        "y_cm = np.ones(len(df))  # Label 1 for code-mixed\n",
        "\n",
        "# === Step 4: Create synthetic non-code-mixed samples (label 0) ===\n",
        "synthetic_english = [\n",
        "    \"This is a nice day.\",\n",
        "    \"I am going to school.\",\n",
        "    \"We had dinner together.\",\n",
        "    \"Let's play football.\",\n",
        "    \"She reads every day.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_telugu = [\n",
        "    \"Nenu school ki veltunna.\",\n",
        "    \"Aame rojuki oka book chadutundi.\",\n",
        "    \"Manam kalisi dinner chesamu.\",\n",
        "    \"Football aadudham.\",\n",
        "    \"Idi chala bagundi.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_sentences = synthetic_english + synthetic_telugu\n",
        "y_synth = [0] * len(synthetic_sentences)\n",
        "X_synth = np.array([extract_features(sent) for sent in synthetic_sentences])\n",
        "\n",
        "# === Step 5: Combine datasets ===\n",
        "X = np.vstack([X_cm, X_synth])\n",
        "y = np.concatenate([y_cm, y_synth])\n",
        "\n",
        "# === Step 6: Train the Classifier ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# === Step 7: Evaluate the Model ===\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# === Optional: Save the model ===\n",
        "joblib.dump(clf, \"code_mixed_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "3bf14424-0384-4cfa-cd1f-ddf1694807bb",
        "id": "zCwCZJMn-C6D"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-6-8876275e1e36>, line 10)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-8876275e1e36>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    df = pd.read_excel(/code_mixed_sentences11.xlsx)  # File from your upload\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Su34hWh-iFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "ZHYgkkNS-vqz",
        "outputId": "bb10c96b-82e3-45f2-8652-82e1d0eeebe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bli4lf5x_AcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from langdetect import detect\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import joblib\n",
        "import re\n",
        "from langdetect import detect\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# === Step 1: Load your uploaded Excel dataset ===\n",
        "df = pd.read_excel('code_mixed_sentences11.xlsx')  # File from your upload\n",
        "print(\"Loaded dataset:\", df.shape)\n",
        "\n",
        "# === Step 2: Language Detection Helper ===\n",
        "def detect_languages(sentence):\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    lang_tags = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            lang = detect(token)\n",
        "        except:\n",
        "            lang = \"unknown\"\n",
        "        lang_tags.append(lang)\n",
        "    return tokens, lang_tags\n",
        "\n",
        "# === Step 3: Feature Extraction Function ===\n",
        "def extract_features(sentence):\n",
        "    tokens, lang_tags = detect_languages(sentence)\n",
        "    total = len(tokens)\n",
        "    en_count = lang_tags.count('en')\n",
        "    other_count = total - en_count\n",
        "    switch_count = sum(1 for i in range(1, len(lang_tags)) if lang_tags[i] != lang_tags[i-1])\n",
        "    return [total, en_count, other_count, switch_count, en_count/total if total else 0]\n",
        "\n",
        "# Apply feature extraction\n",
        "df['features'] = df['CodeMixedSentence'].apply(extract_features)\n",
        "X_cm = np.array(df['features'].tolist())\n",
        "y_cm = np.ones(len(df))  # Label 1 for code-mixed\n",
        "\n",
        "# === Step 4: Create synthetic non-code-mixed samples (label 0) ===\n",
        "synthetic_english = [\n",
        "    \"This is a nice day.\",\n",
        "    \"I am going to school.\",\n",
        "    \"We had dinner together.\",\n",
        "    \"Let's play football.\",\n",
        "    \"She reads every day.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_telugu = [\n",
        "    \"Nenu school ki veltunna.\",\n",
        "    \"Aame rojuki oka book chadutundi.\",\n",
        "    \"Manam kalisi dinner chesamu.\",\n",
        "    \"Football aadudham.\",\n",
        "    \"Idi chala bagundi.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_sentences = synthetic_english + synthetic_telugu\n",
        "y_synth = [0] * len(synthetic_sentences)\n",
        "X_synth = np.array([extract_features(sent) for sent in synthetic_sentences])\n",
        "\n",
        "# === Step 5: Combine datasets ===\n",
        "X = np.vstack([X_cm, X_synth])\n",
        "y = np.concatenate([y_cm, y_synth])\n",
        "\n",
        "# === Step 6: Train the Classifier ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# === Step 7: Evaluate the Model ===\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# === Optional: Save the model ===\n",
        "joblib.dump(clf, \"code_mixed_model.pkl\")\n",
        "# === Step 2: Language Detection Helper ===\n",
        "def detect_languages(sentence):\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    lang_tags = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            lang = detect(token)\n",
        "        except:\n",
        "            lang = \"unknown\"\n",
        "        lang_tags.append(lang)\n",
        "    return tokens, lang_tags\n",
        "\n",
        "# === Step 3: Feature Extraction Function ===\n",
        "def extract_features(sentence):\n",
        "    tokens, lang_tags = detect_languages(sentence)\n",
        "    total = len(tokens)\n",
        "    en_count = lang_tags.count('en')\n",
        "    other_count = total - en_count\n",
        "    switch_count = sum(1 for i in range(1, len(lang_tags)) if lang_tags[i] != lang_tags[i-1])\n",
        "    return [total, en_count, other_count, switch_count, en_count/total if total else 0]\n",
        "\n",
        "# Apply feature extraction\n",
        "df['features'] = df['CodeMixedSentence'].apply(extract_features)\n",
        "X_cm = np.array(df['features'].tolist())\n",
        "y_cm = np.ones(len(df))  # Label 1 for code-mixed\n",
        "\n",
        "# === Step 4: Create synthetic non-code-mixed samples (label 0) ===\n",
        "synthetic_english = [\n",
        "    \"This is a nice day.\",\n",
        "    \"I am going to school.\",\n",
        "    \"We had dinner together.\",\n",
        "    \"Let's play football.\",\n",
        "    \"She reads every day.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_telugu = [\n",
        "    \"Nenu school ki veltunna.\",\n",
        "    \"Aame rojuki oka book chadutundi.\",\n",
        "    \"Manam kalisi dinner chesamu.\",\n",
        "    \"Football aadudham.\",\n",
        "    \"Idi chala bagundi.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_sentences = synthetic_english + synthetic_telugu\n",
        "y_synth = [0] * len(synthetic_sentences)\n",
        "X_synth = np.array([extract_features(sent) for sent in synthetic_sentences])\n",
        "\n",
        "# === Step 5: Combine datasets ===\n",
        "X = np.vstack([X_cm, X_synth])\n",
        "y = np.concatenate([y_cm, y_synth])\n",
        "\n",
        "# === Step 6: Train the Classifier ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# === Step 7: Evaluate the Model ===\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# === Optional: Save the model ===\n",
        "joblib.dump(clf, \"code_mixed_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "2901469c-d023-461b-f9ee-978974068241",
        "id": "NOZb_-58_BO3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'code_mixed_sentences11.xlsx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-cddd5107182b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# === Step 1: Load your uploaded Excel dataset ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'code_mixed_sentences11.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# File from your upload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded dataset:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'code_mixed_sentences11.xlsx'"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install pandas openpyxl scikit-learn langdetect\n",
        "\n",
        "import re\n",
        "from langdetect import detect\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import joblib\n",
        "import os  # Import the os module for file operations\n",
        "import pandas as pd\n",
        "\n",
        "# === Step 1: Load your uploaded Excel dataset ===\n",
        "# Provide the correct path to your Excel file\n",
        "file_path = \"/content/code_mixed_sentences11.xlsx\"  # Update with your file path if necessary\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(file_path):\n",
        "    raise FileNotFoundError(f\"File not found: {file_path}. Please make sure the file is uploaded or the path is correct.\")\n",
        "\n",
        "df = pd.read_excel(file_path)  # File from your upload\n",
        "print(\"Loaded dataset:\", df.shape)\n",
        "\n",
        "# === Step 2: Language Detection Helper ===\n",
        "def detect_languages(sentence):\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    lang_tags = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            lang = detect(token)\n",
        "        except:\n",
        "            lang = \"unknown\"\n",
        "        lang_tags.append(lang)\n",
        "    return tokens, lang_tags\n",
        "\n",
        "# === Step 3: Feature Extraction Function ===\n",
        "def extract_features(sentence):\n",
        "    tokens, lang_tags = detect_languages(sentence)\n",
        "    total = len(tokens)\n",
        "    en_count = lang_tags.count('en')\n",
        "    other_count = total - en_count\n",
        "    switch_count = sum(1 for i in range(1, len(lang_tags)) if lang_tags[i] != lang_tags[i-1])\n",
        "    return [total, en_count, other_count, switch_count, en_count/total if total else 0]\n",
        "\n",
        "# Apply feature extraction\n",
        "df['features'] = df['CodeMixedSentence'].apply(extract_features)\n",
        "X_cm = np.array(df['features'].tolist())\n",
        "y_cm = np.ones(len(df))  # Label 1 for code-mixed\n",
        "\n",
        "# === Step 4: Create synthetic non-code-mixed samples (label 0) ===\n",
        "synthetic_english = [\n",
        "    \"This is a nice day.\",\n",
        "    \"I am going to school.\",\n",
        "    \"We had dinner together.\",\n",
        "    \"Let's play football.\",\n",
        "    \"She reads every day.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_telugu = [\n",
        "    \"Nenu school ki veltunna.\",\n",
        "    \"Aame rojuki oka book chadutundi.\",\n",
        "    \"Manam kalisi dinner chesamu.\",\n",
        "    \"Football aadudham.\",\n",
        "    \"Idi chala bagundi.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_sentences = synthetic_english + synthetic_telugu\n",
        "y_synth = [0] * len(synthetic_sentences)\n",
        "X_synth = np.array([extract_features(sent) for sent in synthetic_sentences])\n",
        "\n",
        "# === Step 5: Combine datasets ===\n",
        "X = np.vstack([X_cm, X_synth])\n",
        "y = np.concatenate([y_cm, y_synth])\n",
        "\n",
        "# === Step 6: Train the Classifier ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# === Step 7: Evaluate the Model ===\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# === Optional: Save the model ===\n",
        "joblib.dump(clf, \"code_mixed_model.pkl\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "gCxaf31ZAJey",
        "outputId": "5eee1006-d3aa-40ce-acdb-b9abc5fcd046",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "File not found: /content/code_mixed_sentences11.xlsx. Please make sure the file is uploaded or the path is correct.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-c4898337550e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Check if the file exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"File not found: {file_path}. Please make sure the file is uploaded or the path is correct.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# File from your upload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: File not found: /content/code_mixed_sentences11.xlsx. Please make sure the file is uploaded or the path is correct."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from langdetect import detect\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import joblib\n",
        "import re\n",
        "from langdetect import detect\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# === Step 1: Load your uploaded Excel dataset ===\n",
        "# Provide the correct path to your Excel file\n",
        "file_path = \"/content/code_mixed_sentences11.xlsx\"  # Update with your file path if necessary\n",
        "df = pd.read_excel(file_path)  # File from your upload\n",
        "print(\"Loaded dataset:\", df.shape)\n",
        "\n",
        "# === Step 2: Language Detection Helper ===\n",
        "# ... (rest of your code remains the same)"
      ],
      "metadata": {
        "id": "gC1KqiDz_dZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from langdetect import detect\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import joblib\n",
        "import re\n",
        "from langdetect import detect\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# === Step 1: Load your uploaded Excel dataset ===\n",
        "df = pd.read_excel('code_mixed_sentences11.xlsx')  # File from your upload\n",
        "print(\"Loaded dataset:\", df.shape)\n",
        "\n",
        "# === Step 2: Language Detection Helper ===\n",
        "def detect_languages(sentence):\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    lang_tags = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            lang = detect(token)\n",
        "        except:\n",
        "            lang = \"unknown\"\n",
        "        lang_tags.append(lang)\n",
        "    return tokens, lang_tags\n",
        "\n",
        "# === Step 3: Feature Extraction Function ===\n",
        "def extract_features(sentence):\n",
        "    tokens, lang_tags = detect_languages(sentence)\n",
        "    total = len(tokens)\n",
        "    en_count = lang_tags.count('en')\n",
        "    other_count = total - en_count\n",
        "    switch_count = sum(1 for i in range(1, len(lang_tags)) if lang_tags[i] != lang_tags[i-1])\n",
        "    return [total, en_count, other_count, switch_count, en_count/total if total else 0]\n",
        "\n",
        "# Apply feature extraction\n",
        "df['features'] = df['CodeMixedSentence'].apply(extract_features)\n",
        "X_cm = np.array(df['features'].tolist())\n",
        "y_cm = np.ones(len(df))  # Label 1 for code-mixed\n",
        "\n",
        "# === Step 4: Create synthetic non-code-mixed samples (label 0) ===\n",
        "synthetic_english = [\n",
        "    \"This is a nice day.\",\n",
        "    \"I am going to school.\",\n",
        "    \"We had dinner together.\",\n",
        "    \"Let's play football.\",\n",
        "    \"She reads every day.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_telugu = [\n",
        "    \"Nenu school ki veltunna.\",\n",
        "    \"Aame rojuki oka book chadutundi.\",\n",
        "    \"Manam kalisi dinner chesamu.\",\n",
        "    \"Football aadudham.\",\n",
        "    \"Idi chala bagundi.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_sentences = synthetic_english + synthetic_telugu\n",
        "y_synth = [0] * len(synthetic_sentences)\n",
        "X_synth = np.array([extract_features(sent) for sent in synthetic_sentences])\n",
        "\n",
        "# === Step 5: Combine datasets ===\n",
        "X = np.vstack([X_cm, X_synth])\n",
        "y = np.concatenate([y_cm, y_synth])\n",
        "\n",
        "# === Step 6: Train the Classifier ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# === Step 7: Evaluate the Model ===\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# === Optional: Save the model ===\n",
        "joblib.dump(clf, \"code_mixed_model.pkl\")\n",
        "# === Step 2: Language Detection Helper ===\n",
        "def detect_languages(sentence):\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    lang_tags = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            lang = detect(token)\n",
        "        except:\n",
        "            lang = \"unknown\"\n",
        "        lang_tags.append(lang)\n",
        "    return tokens, lang_tags\n",
        "\n",
        "# === Step 3: Feature Extraction Function ===\n",
        "def extract_features(sentence):\n",
        "    tokens, lang_tags = detect_languages(sentence)\n",
        "    total = len(tokens)\n",
        "    en_count = lang_tags.count('en')\n",
        "    other_count = total - en_count\n",
        "    switch_count = sum(1 for i in range(1, len(lang_tags)) if lang_tags[i] != lang_tags[i-1])\n",
        "    return [total, en_count, other_count, switch_count, en_count/total if total else 0]\n",
        "\n",
        "# Apply feature extraction\n",
        "df['features'] = df['CodeMixedSentence'].apply(extract_features)\n",
        "X_cm = np.array(df['features'].tolist())\n",
        "y_cm = np.ones(len(df))  # Label 1 for code-mixed\n",
        "\n",
        "# === Step 4: Create synthetic non-code-mixed samples (label 0) ===\n",
        "synthetic_english = [\n",
        "    \"This is a nice day.\",\n",
        "    \"I am going to school.\",\n",
        "    \"We had dinner together.\",\n",
        "    \"Let's play football.\",\n",
        "    \"She reads every day.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_telugu = [\n",
        "    \"Nenu school ki veltunna.\",\n",
        "    \"Aame rojuki oka book chadutundi.\",\n",
        "    \"Manam kalisi dinner chesamu.\",\n",
        "    \"Football aadudham.\",\n",
        "    \"Idi chala bagundi.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_sentences = synthetic_english + synthetic_telugu\n",
        "y_synth = [0] * len(synthetic_sentences)\n",
        "X_synth = np.array([extract_features(sent) for sent in synthetic_sentences])\n",
        "\n",
        "# === Step 5: Combine datasets ===\n",
        "X = np.vstack([X_cm, X_synth])\n",
        "y = np.concatenate([y_cm, y_synth])\n",
        "\n",
        "# === Step 6: Train the Classifier ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# === Step 7: Evaluate the Model ===\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# === Optional: Save the model ===\n",
        "joblib.dump(clf, \"code_mixed_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "40db54a1-9277-49cb-d7d5-95d32d07c45e",
        "id": "C3x4FaPe_irg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'code_mixed_sentences11.xlsx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-cddd5107182b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# === Step 1: Load your uploaded Excel dataset ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'code_mixed_sentences11.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# File from your upload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded dataset:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'code_mixed_sentences11.xlsx'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from langdetect import detect\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import joblib\n",
        "import re\n",
        "from langdetect import detect\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# === Step 1: Load your uploaded Excel dataset ===\n",
        "df = pd.read_excel('code_mixed_sentences11.xlsx')  # File from your upload\n",
        "print(\"Loaded dataset:\", df.shape)\n",
        "\n",
        "# === Step 2: Language Detection Helper ===\n",
        "def detect_languages(sentence):\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    lang_tags = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            lang = detect(token)\n",
        "        except:\n",
        "            lang = \"unknown\"\n",
        "        lang_tags.append(lang)\n",
        "    return tokens, lang_tags\n",
        "\n",
        "# === Step 3: Feature Extraction Function ===\n",
        "def extract_features(sentence):\n",
        "    tokens, lang_tags = detect_languages(sentence)\n",
        "    total = len(tokens)\n",
        "    en_count = lang_tags.count('en')\n",
        "    other_count = total - en_count\n",
        "    switch_count = sum(1 for i in range(1, len(lang_tags)) if lang_tags[i] != lang_tags[i-1])\n",
        "    return [total, en_count, other_count, switch_count, en_count/total if total else 0]\n",
        "\n",
        "# Apply feature extraction\n",
        "df['features'] = df['CodeMixedSentence'].apply(extract_features)\n",
        "X_cm = np.array(df['features'].tolist())\n",
        "y_cm = np.ones(len(df))  # Label 1 for code-mixed\n",
        "\n",
        "# === Step 4: Create synthetic non-code-mixed samples (label 0) ===\n",
        "synthetic_english = [\n",
        "    \"This is a nice day.\",\n",
        "    \"I am going to school.\",\n",
        "    \"We had dinner together.\",\n",
        "    \"Let's play football.\",\n",
        "    \"She reads every day.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_telugu = [\n",
        "    \"Nenu school ki veltunna.\",\n",
        "    \"Aame rojuki oka book chadutundi.\",\n",
        "    \"Manam kalisi dinner chesamu.\",\n",
        "    \"Football aadudham.\",\n",
        "    \"Idi chala bagundi.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_sentences = synthetic_english + synthetic_telugu\n",
        "y_synth = [0] * len(synthetic_sentences)\n",
        "X_synth = np.array([extract_features(sent) for sent in synthetic_sentences])\n",
        "\n",
        "# === Step 5: Combine datasets ===\n",
        "X = np.vstack([X_cm, X_synth])\n",
        "y = np.concatenate([y_cm, y_synth])\n",
        "\n",
        "# === Step 6: Train the Classifier ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# === Step 7: Evaluate the Model ===\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# === Optional: Save the model ===\n",
        "joblib.dump(clf, \"code_mixed_model.pkl\")\n",
        "# === Step 2: Language Detection Helper ===\n",
        "def detect_languages(sentence):\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    lang_tags = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            lang = detect(token)\n",
        "        except:\n",
        "            lang = \"unknown\"\n",
        "        lang_tags.append(lang)\n",
        "    return tokens, lang_tags\n",
        "\n",
        "# === Step 3: Feature Extraction Function ===\n",
        "def extract_features(sentence):\n",
        "    tokens, lang_tags = detect_languages(sentence)\n",
        "    total = len(tokens)\n",
        "    en_count = lang_tags.count('en')\n",
        "    other_count = total - en_count\n",
        "    switch_count = sum(1 for i in range(1, len(lang_tags)) if lang_tags[i] != lang_tags[i-1])\n",
        "    return [total, en_count, other_count, switch_count, en_count/total if total else 0]\n",
        "\n",
        "# Apply feature extraction\n",
        "df['features'] = df['CodeMixedSentence'].apply(extract_features)\n",
        "X_cm = np.array(df['features'].tolist())\n",
        "y_cm = np.ones(len(df))  # Label 1 for code-mixed\n",
        "\n",
        "# === Step 4: Create synthetic non-code-mixed samples (label 0) ===\n",
        "synthetic_english = [\n",
        "    \"This is a nice day.\",\n",
        "    \"I am going to school.\",\n",
        "    \"We had dinner together.\",\n",
        "    \"Let's play football.\",\n",
        "    \"She reads every day.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_telugu = [\n",
        "    \"Nenu school ki veltunna.\",\n",
        "    \"Aame rojuki oka book chadutundi.\",\n",
        "    \"Manam kalisi dinner chesamu.\",\n",
        "    \"Football aadudham.\",\n",
        "    \"Idi chala bagundi.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_sentences = synthetic_english + synthetic_telugu\n",
        "y_synth = [0] * len(synthetic_sentences)\n",
        "X_synth = np.array([extract_features(sent) for sent in synthetic_sentences])\n",
        "\n",
        "# === Step 5: Combine datasets ===\n",
        "X = np.vstack([X_cm, X_synth])\n",
        "y = np.concatenate([y_cm, y_synth])\n",
        "\n",
        "# === Step 6: Train the Classifier ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# === Step 7: Evaluate the Model ===\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# === Optional: Save the model ===\n",
        "joblib.dump(clf, \"code_mixed_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "40db54a1-9277-49cb-d7d5-95d32d07c45e",
        "id": "KJJ_fiIv_kr_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'code_mixed_sentences11.xlsx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-cddd5107182b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# === Step 1: Load your uploaded Excel dataset ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'code_mixed_sentences11.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# File from your upload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded dataset:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'code_mixed_sentences11.xlsx'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from langdetect import detect\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import joblib\n",
        "import re\n",
        "from langdetect import detect\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# === Step 1: Load your uploaded Excel dataset ===\n",
        "# Provide the correct path to your Excel file\n",
        "file_path = \"/content/code_mixed_sentences11.xlsx\"  # Update with your file path if necessary\n",
        "df = pd.read_excel(file_path)  # File from your upload\n",
        "print(\"Loaded dataset:\", df.shape)\n",
        "\n",
        "# === Step 2: Language Detection Helper ===\n",
        "# ... (rest of your code remains the same)\n",
        "\n",
        "\n",
        "# === Step 2: Language Detection Helper ===\n",
        "def detect_languages(sentence):\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    lang_tags = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            lang = detect(token)\n",
        "        except:\n",
        "            lang = \"unknown\"\n",
        "        lang_tags.append(lang)\n",
        "    return tokens, lang_tags\n",
        "\n",
        "# === Step 3: Feature Extraction Function ===\n",
        "def extract_features(sentence):\n",
        "    tokens, lang_tags = detect_languages(sentence)\n",
        "    total = len(tokens)\n",
        "    en_count = lang_tags.count('en')\n",
        "    other_count = total - en_count\n",
        "    switch_count = sum(1 for i in range(1, len(lang_tags)) if lang_tags[i] != lang_tags[i-1])\n",
        "    return [total, en_count, other_count, switch_count, en_count/total if total else 0]\n",
        "\n",
        "# Apply feature extraction\n",
        "df['features'] = df['CodeMixedSentence'].apply(extract_features)\n",
        "X_cm = np.array(df['features'].tolist())\n",
        "y_cm = np.ones(len(df))  # Label 1 for code-mixed\n",
        "\n",
        "# === Step 4: Create synthetic non-code-mixed samples (label 0) ===\n",
        "synthetic_english = [\n",
        "    \"This is a nice day.\",\n",
        "    \"I am going to school.\",\n",
        "    \"We had dinner together.\",\n",
        "    \"Let's play football.\",\n",
        "    \"She reads every day.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_telugu = [\n",
        "    \"Nenu school ki veltunna.\",\n",
        "    \"Aame rojuki oka book chadutundi.\",\n",
        "    \"Manam kalisi dinner chesamu.\",\n",
        "    \"Football aadudham.\",\n",
        "    \"Idi chala bagundi.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_sentences = synthetic_english + synthetic_telugu\n",
        "y_synth = [0] * len(synthetic_sentences)\n",
        "X_synth = np.array([extract_features(sent) for sent in synthetic_sentences])\n",
        "\n",
        "# === Step 5: Combine datasets ===\n",
        "X = np.vstack([X_cm, X_synth])\n",
        "y = np.concatenate([y_cm, y_synth])\n",
        "\n",
        "# === Step 6: Train the Classifier ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# === Step 7: Evaluate the Model ===\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# === Optional: Save the model ===\n",
        "joblib.dump(clf, \"code_mixed_model.pkl\")\n",
        "# === Step 2: Language Detection Helper ===\n",
        "def detect_languages(sentence):\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    lang_tags = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            lang = detect(token)\n",
        "        except:\n",
        "            lang = \"unknown\"\n",
        "        lang_tags.append(lang)\n",
        "    return tokens, lang_tags\n",
        "\n",
        "# === Step 3: Feature Extraction Function ===\n",
        "def extract_features(sentence):\n",
        "    tokens, lang_tags = detect_languages(sentence)\n",
        "    total = len(tokens)\n",
        "    en_count = lang_tags.count('en')\n",
        "    other_count = total - en_count\n",
        "    switch_count = sum(1 for i in range(1, len(lang_tags)) if lang_tags[i] != lang_tags[i-1])\n",
        "    return [total, en_count, other_count, switch_count, en_count/total if total else 0]\n",
        "\n",
        "# Apply feature extraction\n",
        "df['features'] = df['CodeMixedSentence'].apply(extract_features)\n",
        "X_cm = np.array(df['features'].tolist())\n",
        "y_cm = np.ones(len(df))  # Label 1 for code-mixed\n",
        "\n",
        "# === Step 4: Create synthetic non-code-mixed samples (label 0) ===\n",
        "synthetic_english = [\n",
        "    \"This is a nice day.\",\n",
        "    \"I am going to school.\",\n",
        "    \"We had dinner together.\",\n",
        "    \"Let's play football.\",\n",
        "    \"She reads every day.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_telugu = [\n",
        "    \"Nenu school ki veltunna.\",\n",
        "    \"Aame rojuki oka book chadutundi.\",\n",
        "    \"Manam kalisi dinner chesamu.\",\n",
        "    \"Football aadudham.\",\n",
        "    \"Idi chala bagundi.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_sentences = synthetic_english + synthetic_telugu\n",
        "y_synth = [0] * len(synthetic_sentences)\n",
        "X_synth = np.array([extract_features(sent) for sent in synthetic_sentences])\n",
        "\n",
        "# === Step 5: Combine datasets ===\n",
        "X = np.vstack([X_cm, X_synth])\n",
        "y = np.concatenate([y_cm, y_synth])\n",
        "\n",
        "# === Step 6: Train the Classifier ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# === Step 7: Evaluate the Model ===\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# === Optional: Save the model ===\n",
        "joblib.dump(clf, \"code_mixed_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "b278a139-7c87-42e8-9306-ba34adcbbde4",
        "id": "0qhfECKz_ktj"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset: (413, 1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected string or bytes-like object, got 'float'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-6533a1cd07ef>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# Apply feature extraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CodeMixedSentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0mX_cm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0my_cm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Label 1 for code-mixed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-6533a1cd07ef>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# === Step 3: Feature Extraction Function ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_languages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0men_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang_tags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-6533a1cd07ef>\u001b[0m in \u001b[0;36mdetect_languages\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# === Step 2: Language Detection Helper ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetect_languages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\w+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mlang_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/re/__init__.py\u001b[0m in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     Empty matches are included in the result.\"\"\"\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object, got 'float'"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install pandas openpyxl scikit-learn langdetect\n",
        "\n",
        "import re\n",
        "from langdetect import detect\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import joblib\n",
        "import os  # Import the os module for file operations\n",
        "import pandas as pd\n",
        "\n",
        "# === Step 1: Load your uploaded Excel dataset ===\n",
        "# Provide the correct path to your Excel file\n",
        "file_path = \"/content/code_mixed_sentences11.xlsx\"  # Update with your file path if necessary\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(file_path):\n",
        "    raise FileNotFoundError(f\"File not found: {file_path}. Please make sure the file is uploaded or the path is correct.\")\n",
        "\n",
        "df = pd.read_excel(file_path)  # File from your upload\n",
        "print(\"Loaded dataset:\", df.shape)\n",
        "\n",
        "# === Step 2: Language Detection Helper ===\n",
        "def detect_languages(sentence):\n",
        "    # Convert sentence to string if it's not already\n",
        "    sentence = str(sentence)  # This line is added to handle non-string values\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    lang_tags = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            lang = detect(token)\n",
        "        except:\n",
        "            lang = \"unknown\"\n",
        "        lang_tags.append(lang)\n",
        "    return tokens, lang_tags\n",
        "\n",
        "# === Step 3: Feature Extraction Function ===\n",
        "def extract_features(sentence):\n",
        "    tokens, lang_tags = detect_languages(sentence)\n",
        "    total = len(tokens)\n",
        "    en_count = lang_tags.count('en')\n",
        "    other_count = total - en_count\n",
        "    switch_count = sum(1 for i in range(1, len(lang_tags)) if lang_tags[i] != lang_tags[i-1])\n",
        "    return [total, en_count, other_count, switch_count, en_count/total if total else 0]\n",
        "\n",
        "# Apply feature extraction\n",
        "df['features'] = df['CodeMixedSentence'].apply(extract_features)\n",
        "X_cm = np.array(df['features'].tolist())\n",
        "y_cm = np.ones(len(df))  # Label 1 for code-mixed\n",
        "\n",
        "# === Step 4: Create synthetic non-code-mixed samples (label 0) ===\n",
        "synthetic_english = [\n",
        "    \"This is a nice day.\",\n",
        "    \"I am going to school.\",\n",
        "    \"We had dinner together.\",\n",
        "    \"Let's play football.\",\n",
        "    \"She reads every day.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_telugu = [\n",
        "    \"Nenu school ki veltunna.\",\n",
        "    \"Aame rojuki oka book chadutundi.\",\n",
        "    \"Manam kalisi dinner chesamu.\",\n",
        "    \"Football aadudham.\",\n",
        "    \"Idi chala bagundi.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_sentences = synthetic_english + synthetic_telugu\n",
        "y_synth = [0] * len(synthetic_sentences)\n",
        "X_synth = np.array([extract_features(sent) for sent in synthetic_sentences])\n",
        "\n",
        "# === Step 5: Combine datasets ===\n",
        "X = np.vstack([X_cm, X_synth])\n",
        "y = np.concatenate([y_cm, y_synth])\n",
        "\n",
        "# === Step 6: Train the Classifier ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# === Step 7: Evaluate the Model ===\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# === Optional: Save the model ===\n",
        "joblib.dump(clf, \"code_mixed_model.pkl\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqL_zcaSAzMX",
        "outputId": "1877308d-c69b-4355-a80e-67e914806071"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Loaded dataset: (413, 1)\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.89      1.00      0.94       157\n",
            "         1.0       1.00      0.79      0.88        90\n",
            "\n",
            "    accuracy                           0.92       247\n",
            "   macro avg       0.95      0.89      0.91       247\n",
            "weighted avg       0.93      0.92      0.92       247\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['code_mixed_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 8: Predict on Original Data ===\n",
        "df['PredictedLabel'] = clf.predict(X_cm)  # Only on original code-mixed sentences\n",
        "\n",
        "# === Step 9: Save to Excel ===\n",
        "output_path = \"predicted_code_mixed_sentences.xlsx\"\n",
        "df.to_excel(output_path, index=False)\n",
        "\n",
        "print(f\"✅ Predictions saved to: {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziHmxiUzBdlh",
        "outputId": "b693d6d5-3b96-4da3-a6b4-3364b76f6a69"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Predictions saved to: predicted_code_mixed_sentences.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# === Step 10.1: Confusion Matrix ===\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "labels = ['Not Code-Mixed', 'Code-Mixed']\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# === Step 10.2: Bar Chart of Predictions ===\n",
        "unique, counts = np.unique(y_pred, return_counts=True)\n",
        "plt.figure(figsize=(5,3))\n",
        "sns.barplot(x=labels, y=counts)\n",
        "plt.title(\"Predicted Code-Mixed vs Not Code-Mixed Sentences\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "PMAwSv1SB3S8",
        "outputId": "381e1205-bec9-42a5-f113-c6de6851ea0a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit langdetect joblib pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb-enajVCIW1",
        "outputId": "e20c0241-ba1c-45bc-de5a-96db3f4c7127"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.44.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.35.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Downloading streamlit-1.44.1-py3-none-any.whl (9.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.44.1 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import re\n",
        "from langdetect import detect\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "# Load the trained model\n",
        "model = joblib.load(\"code_mixed_model.pkl\")\n",
        "\n",
        "# Feature extraction (same as before)\n",
        "def detect_languages(sentence):\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    lang_tags = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            lang = detect(token)\n",
        "        except:\n",
        "            lang = \"unknown\"\n",
        "        lang_tags.append(lang)\n",
        "    return tokens, lang_tags\n",
        "\n",
        "def extract_features(sentence):\n",
        "    tokens, lang_tags = detect_languages(sentence)\n",
        "    total = len(tokens)\n",
        "    en_count = lang_tags.count('en')\n",
        "    other_count = total - en_count\n",
        "    switch_count = sum(1 for i in range(1, len(lang_tags)) if lang_tags[i] != lang_tags[i-1])\n",
        "    return [total, en_count, other_count, switch_count, en_count / total if total else 0]\n",
        "\n",
        "# Streamlit App\n",
        "st.title(\"🌐 Code-Mixed Sentence Detector\")\n",
        "st.write(\"Enter a sentence to check if it's code-mixed (English-Telugu)\")\n",
        "\n",
        "sentence = st.text_input(\"Your Sentence:\")\n",
        "\n",
        "if sentence:\n",
        "    features = np.array([extract_features(sentence)])\n",
        "    prediction = model.predict(features)[0]\n",
        "    label = \"✅ Code-Mixed\" if prediction == 1 else \"❌ Not Code-Mixed\"\n",
        "    st.markdown(f\"### Result: {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1mCKKc9CS1c",
        "outputId": "9de3ebda-e158-4acc-e9b8-7cb538626352"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-04-23 19:04:10.639 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:04:10.953 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-04-23 19:04:10.961 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:04:10.965 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:04:10.966 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:04:10.966 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:04:10.970 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:04:10.971 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:04:10.972 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:04:10.975 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:04:10.976 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:04:10.977 Session state does not function when running a script without `streamlit run`\n",
            "2025-04-23 19:04:10.977 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:04:10.978 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.subheader(\"/content/code_mixed_sentences11.xlsx\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload Excel file (.xlsx)\", type=[\"xlsx\"])\n",
        "\n",
        "if uploaded_file:\n",
        "    try:\n",
        "        # Load uploaded Excel\n",
        "        input_df = pd.read_excel(uploaded_file)\n",
        "\n",
        "        if 'CodeMixedSentence' not in input_df.columns:\n",
        "            st.error(\"Excel must have a column named 'CodeMixedSentence'\")\n",
        "        else:\n",
        "            # Extract features for all rows\n",
        "            input_df['features'] = input_df['CodeMixedSentence'].apply(extract_features)\n",
        "            features_array = np.array(input_df['features'].tolist())\n",
        "\n",
        "            # Predict using model\n",
        "            input_df['PredictedLabel'] = model.predict(features_array)\n",
        "            input_df.drop(columns=['features'], inplace=True)\n",
        "\n",
        "            # Show result\n",
        "            st.success(\"Prediction complete ✅\")\n",
        "            st.dataframe(input_df[['CodeMixedSentence', 'PredictedLabel']])\n",
        "\n",
        "            # Convert to Excel for download\n",
        "            def convert_df(df):\n",
        "                return df.to_excel(index=False, engine='openpyxl')\n",
        "\n",
        "            output_xlsx = convert_df(input_df)\n",
        "            st.download_button(\n",
        "                label=\"📥 Download Results as Excel\",\n",
        "                data=output_xlsx,\n",
        "                file_name=\"predicted_output.xlsx\",\n",
        "                mime=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n",
        "            )\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error: {str(e)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-1sixilCil0",
        "outputId": "452a817f-e281-40f9-fce8-bedfcaab70af"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-04-23 19:06:51.529 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:06:51.531 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:06:51.532 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:06:51.535 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:06:51.537 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:06:51.539 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:06:51.541 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:06:51.543 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:06:51.544 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_KKn9Z2CwKU",
        "outputId": "b4f1c93d-f7e6-4def-f4bd-35e913d9882f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: streamlit run [OPTIONS] TARGET [ARGS]...\n",
            "Try 'streamlit run --help' for help.\n",
            "\n",
            "Error: Invalid value: File does not exist: app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "st.markdown(\"---\")\n",
        "st.subheader(\"🔍 Word-Level Language Split\")\n",
        "\n",
        "word_level_input = st.text_input(\"Enter a code-mixed sentence for word-level analysis:\")\n",
        "\n",
        "if word_level_input:\n",
        "    tokens = re.findall(r'\\w+', word_level_input)\n",
        "    en_words = []\n",
        "    te_words = []\n",
        "\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            lang = detect(token)\n",
        "            if lang == 'en':\n",
        "                en_words.append(token)\n",
        "            else:\n",
        "                te_words.append(token)\n",
        "        except:\n",
        "            te_words.append(token)  # fallback\n",
        "\n",
        "    col1, col2 = st.columns(2)\n",
        "    with col1:\n",
        "        st.success(\"🟦 English Words\")\n",
        "        st.write(\", \".join(en_words) if en_words else \"None\")\n",
        "    with col2:\n",
        "        st.warning(\"🔶 Telugu / Other Words\")\n",
        "        st.write(\", \".join(te_words) if te_words else \"None\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3FDDRp_DvQM",
        "outputId": "89dbcb43-eb90-4f00-ce50-50ef6e2752f8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-04-23 19:10:50.903 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:10:50.905 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:10:50.907 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:10:50.909 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:10:50.911 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:10:50.912 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:10:50.913 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:10:50.914 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:10:50.916 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:10:50.919 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import re\n",
        "from langdetect import detect\n",
        "\n",
        "st.title(\"🧠 Code-Mixed Sentence Analyzer\")\n",
        "\n",
        "st.subheader(\"📝 Enter a sentence (English + Telugu):\")\n",
        "sentence = st.text_input(\"Type your sentence here:\")\n",
        "\n",
        "if sentence:\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    en_words = []\n",
        "    te_words = []\n",
        "\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            lang = detect(token)\n",
        "            if lang == 'en':\n",
        "                en_words.append(token)\n",
        "            else:\n",
        "                te_words.append(token)\n",
        "        except:\n",
        "            te_words.append(token)\n",
        "\n",
        "    st.markdown(\"### 🔍 Word-Level Analysis\")\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    with col1:\n",
        "        st.success(\"🟦 English Words\")\n",
        "        st.write(\", \".join(en_words) if en_words else \"None\")\n",
        "\n",
        "    with col2:\n",
        "        st.warning(\"🔶 Telugu / Other Words\")\n",
        "        st.write(\", \".join(te_words) if te_words else \"None\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Wlf3Sl0D5kB",
        "outputId": "fc9a7d9e-bca9-452a-c10c-cc16bf9726ce"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-04-23 19:11:08.985 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:11:08.994 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:11:08.998 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:11:09.002 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:11:09.006 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:11:09.010 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:11:09.014 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:11:09.018 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:11:09.020 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-23 19:11:09.024 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect\n",
        "\n",
        "from langdetect import detect\n",
        "import re\n",
        "\n",
        "def split_english_telugu(sentence):\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    en_words = []\n",
        "    te_words = []\n",
        "\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            lang = detect(token)\n",
        "            if lang == 'en':\n",
        "                en_words.append(token)\n",
        "            else:\n",
        "                te_words.append(token)\n",
        "        except:\n",
        "            te_words.append(token)\n",
        "\n",
        "    print(\"🟦 English Words:\")\n",
        "    print(\", \".join(en_words) if en_words else \"None\")\n",
        "\n",
        "    print(\"\\n🔶 Telugu / Other Words:\")\n",
        "    print(\", \".join(te_words) if te_words else \"None\")\n",
        "\n",
        "# 🔽 Run this cell each time with a new sentence\n",
        "input_sentence = input(\"Enter a code-mixed sentence (English + Telugu): \")\n",
        "split_english_telugu(input_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5X2qsHTlECfS",
        "outputId": "d5b2f8da-455b-42eb-b9a0-202a84eb6d47"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Enter a code-mixed sentence (English + Telugu): hello yela unnav\n",
            "🟦 English Words:\n",
            "None\n",
            "\n",
            "🔶 Telugu / Other Words:\n",
            "hello, yela, unnav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langdetect import detect\n",
        "import re\n",
        "\n",
        "# Take input\n",
        "sentence = input(\"Enter your code-mixed sentence (English + Telugu): \")\n",
        "\n",
        "tokens = re.findall(r'\\w+', sentence)\n",
        "en_words = []\n",
        "te_words = []\n",
        "\n",
        "for token in tokens:\n",
        "    try:\n",
        "        lang = detect(token)\n",
        "        if lang == 'en':\n",
        "            en_words.append(token)\n",
        "        else:\n",
        "            te_words.append(token)\n",
        "    except:\n",
        "        te_words.append(token)\n",
        "\n",
        "print(\"\\n🟦 English Words:\")\n",
        "print(en_words)\n",
        "\n",
        "print(\"\\n🔶 Telugu / Other Words:\")\n",
        "print(te_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GigaG0LFEcnM",
        "outputId": "91381d33-74f4-477a-d848-b4c0fde7f1b2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your code-mixed sentence (English + Telugu): hello yela\n",
            "\n",
            "🟦 English Words:\n",
            "[]\n",
            "\n",
            "🔶 Telugu / Other Words:\n",
            "['hello', 'yela']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import re\n",
        "from langdetect import detect\n",
        "\n",
        "def classify_words(sentence):\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    en_words = []\n",
        "    te_words = []\n",
        "\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            lang = detect(token)\n",
        "            if lang == 'en':\n",
        "                en_words.append(token)\n",
        "            else:\n",
        "                te_words.append(token)\n",
        "        except:\n",
        "            te_words.append(token)\n",
        "\n",
        "    print(\"\\n🟦 English Words:\")\n",
        "    print(\", \".join(en_words) if en_words else \"None\")\n",
        "\n",
        "    print(\"\\n🔶 Telugu / Other Words:\")\n",
        "    print(\", \".join(te_words) if te_words else \"None\")\n",
        "\n",
        "# Textbox widget\n",
        "text_box = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Type a code-mixed sentence...',\n",
        "    description='Sentence:',\n",
        "    disabled=False,\n",
        "    layout=widgets.Layout(width='80%')\n",
        ")\n",
        "\n",
        "button = widgets.Button(description=\"Analyze\")\n",
        "\n",
        "def on_button_clicked(b):\n",
        "    classify_words(text_box.value)\n",
        "\n",
        "button.on_click(on_button_clicked)\n",
        "\n",
        "display(text_box, button)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "4c64ba4dd4834af9a57d1c6a08fd9137",
            "165e029635f145ae978e1598c8d12c03",
            "2632692bb57f4142a9fd9eedcbde9e0f",
            "1449a45667a24a71980e4b3564ba7458",
            "e6159be2aa8245c5988c54f9eab0b801",
            "31fb56662fa849cf8ae5f2fd25197eff"
          ]
        },
        "id": "WywSpL4_Eg58",
        "outputId": "578f38fa-6c49-41ff-837d-7a72e4b7c448"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', description='Sentence:', layout=Layout(width='80%'), placeholder='Type a code-mixed sentence...…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c64ba4dd4834af9a57d1c6a08fd9137"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='Analyze', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1449a45667a24a71980e4b3564ba7458"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟦 English Words:\n",
            "None\n",
            "\n",
            "🔶 Telugu / Other Words:\n",
            "hello, yela, unnav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import re\n",
        "from langdetect import detect, LangDetectException\n",
        "\n",
        "def classify_words(sentence):\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    en_words = []\n",
        "    te_words = []\n",
        "\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            lang = detect(token)\n",
        "            # Check if the language is English\n",
        "            if lang == 'en':\n",
        "                en_words.append(token)\n",
        "            # If it's not detected as English, assume it's Telugu or another language\n",
        "            else:\n",
        "                te_words.append(token)\n",
        "        except LangDetectException:\n",
        "            # If langdetect throws an exception, consider the word as Telugu/other\n",
        "            te_words.append(token)\n",
        "\n",
        "    print(\"\\n🟦 English Words:\")\n",
        "    print(\", \".join(en_words) if en_words else \"None\")\n",
        "\n",
        "    print(\"\\n🔶 Telugu / Other Words:\")\n",
        "    print(\", \".join(te_words) if te_words else \"None\")\n",
        "\n",
        "# Textbox widget\n",
        "text_box = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Type a code-mixed sentence...',\n",
        "    description='Sentence:',\n",
        "    disabled=False,\n",
        "    layout=widgets.Layout(width='80%')\n",
        ")\n",
        "\n",
        "button = widgets.Button(description=\"Analyze\")\n",
        "\n",
        "def on_button_clicked(b):\n",
        "    classify_words(text_box.value)\n",
        "\n",
        "button.on_click(on_button_clicked)\n",
        "\n",
        "display(text_box, button)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "0445227a11f044f8bf6e4eee6697fd4e",
            "adc395ed5c8d4ee5ae5eec7e8d5791ce",
            "9dd4d1372b7b4f7992447b251637b2ca",
            "f2881e7b6ef5461b9d83aa44a3e9a342",
            "34614a79812447aea9ecc9ade7db65fd",
            "73abf9c539f64b879086464a42135460"
          ]
        },
        "id": "RNoz5CMrFd2o",
        "outputId": "29b1c00e-f9c7-46a1-90b0-373dd93705cc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', description='Sentence:', layout=Layout(width='80%'), placeholder='Type a code-mixed sentence...…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0445227a11f044f8bf6e4eee6697fd4e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='Analyze', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2881e7b6ef5461b9d83aa44a3e9a342"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟦 English Words:\n",
            "None\n",
            "\n",
            "🔶 Telugu / Other Words:\n",
            "hello, yela, unnav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import re\n",
        "from langdetect import detect, LangDetectException\n",
        "\n",
        "# Define a list of common English words to help with classification\n",
        "common_english_words = set([\n",
        "    \"hello\", \"hi\", \"thanks\", \"bye\", \"good\", \"morning\", \"evening\", \"please\", \"sorry\", \"welcome\"\n",
        "])\n",
        "\n",
        "def classify_words(sentence):\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    en_words = []\n",
        "    te_words = []\n",
        "\n",
        "    for token in tokens:\n",
        "        # First, check if the token is in the list of common English words\n",
        "        if token.lower() in common_english_words:\n",
        "            en_words.append(token)\n",
        "        else:\n",
        "            try:\n",
        "                lang = detect(token)\n",
        "                if lang == 'en':\n",
        "                    en_words.append(token)\n",
        "                else:\n",
        "                    te_words.append(token)\n",
        "            except LangDetectException:\n",
        "                # If detection fails, consider the word as Telugu/Other\n",
        "                te_words.append(token)\n",
        "\n",
        "    print(\"\\n🟦 English Words:\")\n",
        "    print(\", \".join(en_words) if en_words else \"None\")\n",
        "\n",
        "    print(\"\\n🔶 Telugu / Other Words:\")\n",
        "    print(\", \".join(te_words) if te_words else \"None\")\n",
        "\n",
        "# Textbox widget\n",
        "text_box = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Type a code-mixed sentence...',\n",
        "    description='Sentence:',\n",
        "    disabled=False,\n",
        "    layout=widgets.Layout(width='80%')\n",
        ")\n",
        "\n",
        "button = widgets.Button(description=\"Analyze\")\n",
        "\n",
        "def on_button_clicked(b):\n",
        "    classify_words(text_box.value)\n",
        "\n",
        "button.on_click(on_button_clicked)\n",
        "\n",
        "display(text_box, button)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "041ddb67a088416783fa9556b474e1b3",
            "b8040622e53942d1ab5b582695c136d3",
            "b5dee9c671d4462dbed8ce4e2bb82b66",
            "17439cddce6e4a23a619b6991348406f",
            "6c56a930c1d94830b5c2d01f9e840b1f",
            "b96f7d052def4d8e9c9ecbc59f3807a1"
          ]
        },
        "id": "j6id5AZxFsGF",
        "outputId": "5e3cd1fe-d334-4640-fa20-a9c42880bd72"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', description='Sentence:', layout=Layout(width='80%'), placeholder='Type a code-mixed sentence...…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "041ddb67a088416783fa9556b474e1b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='Analyze', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17439cddce6e4a23a619b6991348406f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟦 English Words:\n",
            "hello\n",
            "\n",
            "🔶 Telugu / Other Words:\n",
            "yela, unnav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import re\n",
        "from langdetect import detect, LangDetectException\n",
        "\n",
        "# Define a list of common English words to help with classification\n",
        "common_english_words = set([\n",
        "    \"hello\", \"hi\", \"thanks\", \"bye\", \"good\", \"morning\", \"evening\", \"please\", \"sorry\", \"welcome\"\n",
        "])\n",
        "\n",
        "def classify_words(sentence):\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    en_words = []\n",
        "    te_words = []\n",
        "\n",
        "    for token in tokens:\n",
        "        # First, check if the token is in the list of common English words\n",
        "        if token.lower() in common_english_words:\n",
        "            en_words.append(token)\n",
        "        else:\n",
        "            try:\n",
        "                lang = detect(token)\n",
        "                if lang == 'en':\n",
        "                    en_words.append(token)\n",
        "                else:\n",
        "                    te_words.append(token)\n",
        "            except LangDetectException:\n",
        "                # If detection fails, consider the word as Telugu/Other\n",
        "                te_words.append(token)\n",
        "\n",
        "    print(\"\\n🟦 English Words:\")\n",
        "    print(\", \".join(en_words) if en_words else \"None\")\n",
        "\n",
        "    print(\"\\n🔶 Telugu / Other Words:\")\n",
        "    print(\", \".join(te_words) if te_words else \"None\")\n",
        "\n",
        "# Textbox widget\n",
        "text_box = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Type a code-mixed sentence...',\n",
        "    description='Sentence:',\n",
        "    disabled=False,\n",
        "    layout=widgets.Layout(width='80%')\n",
        ")\n",
        "\n",
        "button = widgets.Button(description=\"Analyze\")\n",
        "\n",
        "def on_button_clicked(b):\n",
        "    classify_words(text_box.value)\n",
        "\n",
        "button.on_click(on_button_clicked)\n",
        "\n",
        "display(text_box, button)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "041ddb67a088416783fa9556b474e1b3",
            "b8040622e53942d1ab5b582695c136d3",
            "b5dee9c671d4462dbed8ce4e2bb82b66",
            "17439cddce6e4a23a619b6991348406f",
            "6c56a930c1d94830b5c2d01f9e840b1f",
            "b96f7d052def4d8e9c9ecbc59f3807a1"
          ]
        },
        "outputId": "5e3cd1fe-d334-4640-fa20-a9c42880bd72",
        "id": "Sk3vQHOIF9ih"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', description='Sentence:', layout=Layout(width='80%'), placeholder='Type a code-mixed sentence...…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "041ddb67a088416783fa9556b474e1b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='Analyze', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17439cddce6e4a23a619b6991348406f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟦 English Words:\n",
            "hello\n",
            "\n",
            "🔶 Telugu / Other Words:\n",
            "yela, unnav\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install pandas openpyxl scikit-learn langdetect\n",
        "\n",
        "import re\n",
        "from langdetect import detect\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import joblib\n",
        "import os  # Import the os module for file operations\n",
        "import pandas as pd\n",
        "\n",
        "# === Step 1: Load your uploaded Excel dataset ===\n",
        "# Provide the correct path to your Excel file\n",
        "file_path = \"/content/code_mixed_sentences11.xlsx\"  # Update with your file path if necessary\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(file_path):\n",
        "    raise FileNotFoundError(f\"File not found: {file_path}. Please make sure the file is uploaded or the path is correct.\")\n",
        "\n",
        "df = pd.read_excel(file_path)  # File from your upload\n",
        "print(\"Loaded dataset:\", df.shape)\n",
        "\n",
        "# === Step 2: Language Detection Helper ===\n",
        "def detect_languages(sentence):\n",
        "    # Convert sentence to string if it's not already\n",
        "    sentence = str(sentence)  # This line is added to handle non-string values\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    lang_tags = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            lang = detect(token)\n",
        "        except:\n",
        "            lang = \"unknown\"\n",
        "        lang_tags.append(lang)\n",
        "    return tokens, lang_tags\n",
        "\n",
        "# === Step 3: Feature Extraction Function ===\n",
        "def extract_features(sentence):\n",
        "    tokens, lang_tags = detect_languages(sentence)\n",
        "    total = len(tokens)\n",
        "    en_count = lang_tags.count('en')\n",
        "    other_count = total - en_count\n",
        "    switch_count = sum(1 for i in range(1, len(lang_tags)) if lang_tags[i] != lang_tags[i-1])\n",
        "    return [total, en_count, other_count, switch_count, en_count/total if total else 0]\n",
        "\n",
        "# Apply feature extraction\n",
        "df['features'] = df['CodeMixedSentence'].apply(extract_features)\n",
        "X_cm = np.array(df['features'].tolist())\n",
        "y_cm = np.ones(len(df))  # Label 1 for code-mixed\n",
        "\n",
        "# === Step 4: Create synthetic non-code-mixed samples (label 0) ===\n",
        "synthetic_english = [\n",
        "    \"This is a nice day.\",\n",
        "    \"I am going to school.\",\n",
        "    \"We had dinner together.\",\n",
        "    \"Let's play football.\",\n",
        "    \"She reads every day.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_telugu = [\n",
        "    \"Nenu school ki veltunna.\",\n",
        "    \"Aame rojuki oka book chadutundi.\",\n",
        "    \"Manam kalisi dinner chesamu.\",\n",
        "    \"Football aadudham.\",\n",
        "    \"Idi chala bagundi.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_sentences = synthetic_english + synthetic_telugu\n",
        "y_synth = [0] * len(synthetic_sentences)\n",
        "X_synth = np.array([extract_features(sent) for sent in synthetic_sentences])\n",
        "\n",
        "# === Step 5: Combine datasets ===\n",
        "X = np.vstack([X_cm, X_synth])\n",
        "y = np.concatenate([y_cm, y_synth])\n",
        "\n",
        "# === Step 6: Train the Classifier ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# === Step 7: Evaluate the Model ===\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# === Optional: Save the model ===\n",
        "joblib.dump(clf, \"code_mixed_model.pkl\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkS5M7QEGOzq",
        "outputId": "f1d2e22f-f9e8-4586-c4fd-bf7738d2571b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Loaded dataset: (413, 1)\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      1.00      0.94       157\n",
            "         1.0       1.00      0.77      0.87        90\n",
            "\n",
            "    accuracy                           0.91       247\n",
            "   macro avg       0.94      0.88      0.90       247\n",
            "weighted avg       0.93      0.91      0.91       247\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['code_mixed_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "source": [
        "!pip install pandas openpyxl scikit-learn langdetect\n",
        "\n",
        "import re\n",
        "from langdetect import detect\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import joblib\n",
        "import os  # Import the os module for file operations\n",
        "import pandas as pd\n",
        "\n",
        "!pip install pandas openpyxl scikit-learn langdetect\n",
        "\n",
        "import re\n",
        "from langdetect import detect\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import joblib\n",
        "import os  # Import the os module for file operations\n",
        "import pandas as pd\n",
        "\n",
        "# === Step 1: Load your uploaded Excel dataset ===\n",
        "# Provide the correct path to your Excel file\n",
        "file_path = \"/content/code_mixed_sentences11.xlsx\"  # Update with your file path if necessary\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(file_path):\n",
        "    raise FileNotFoundError(f\"File not found: {file_path}. Please make sure the file is uploaded or the path is correct.\")\n",
        "\n",
        "df = pd.read_excel(file_path)  # File from your upload\n",
        "print(\"Loaded dataset:\", df.shape)\n",
        "\n",
        "# === Step 2: Language Detection Helper ===\n",
        "def detect_languages(sentence):\n",
        "    # Convert sentence to string if it's not already\n",
        "    sentence = str(sentence)  # This line is added to handle non-string values\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    lang_tags = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            lang = detect(token)\n",
        "        except:\n",
        "            lang = \"unknown\"\n",
        "        lang_tags.append(lang)\n",
        "    return tokens, lang_tags\n",
        "\n",
        "# === Step 3: Feature Extraction Function ===\n",
        "def extract_features(sentence):\n",
        "    tokens, lang_tags = detect_languages(sentence)\n",
        "    total = len(tokens)\n",
        "    en_count = lang_tags.count('en')\n",
        "    other_count = total - en_count\n",
        "    switch_count = sum(1 for i in range(1, len(lang_tags)) if lang_tags[i] != lang_tags[i-1])\n",
        "    return [total, en_count, other_count, switch_count, en_count/total if total else 0]\n",
        "\n",
        "# Apply feature extraction\n",
        "df['features'] = df['CodeMixedSentence'].apply(extract_features)\n",
        "X_cm = np.array(df['features'].tolist())\n",
        "y_cm = np.ones(len(df))  # Label 1 for code-mixed\n",
        "\n",
        "# === Step 4: Create synthetic non-code-mixed samples (label 0) ===\n",
        "synthetic_english = [\n",
        "    \"This is a nice day.\",\n",
        "    \"I am going to school.\",\n",
        "    \"We had dinner together.\",\n",
        "    \"Let's play football.\",\n",
        "    \"She reads every day.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_telugu = [\n",
        "    \"Nenu school ki veltunna.\",\n",
        "    \"Aame rojuki oka book chadutundi.\",\n",
        "    \"Manam kalisi dinner chesamu.\",\n",
        "    \"Football aadudham.\",\n",
        "    \"Idi chala bagundi.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_sentences = synthetic_english + synthetic_telugu\n",
        "y_synth = [0] * len(synthetic_sentences)\n",
        "X_synth = np.array([extract_features(sent) for sent in synthetic_sentences])\n",
        "\n",
        "# === Step 5: Combine datasets ===\n",
        "X = np.vstack([X_cm, X_synth])\n",
        "y = np.concatenate([y_cm, y_synth])\n",
        "\n",
        "# === Step 6: Train the Classifier ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# === Step 7: Evaluate the Model ===\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# === Optional: Save the model ===\n",
        "joblib.dump(clf, \"code_mixed_model.pkl\")\n",
        "\n",
        "def split_english_telugu(sentence):\n",
        "    \"\"\"Splits a sentence into English and Telugu/Other words.\"\"\"\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    en_words = []\n",
        "    te_words = []\n",
        "\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            lang = detect(token)\n",
        "            if lang == 'en':\n",
        "                en_words.append(token)\n",
        "            else:\n",
        "                te_words.append(token)  # Assume non-English as Telugu/Other\n",
        "        except:\n",
        "            te_words.append(token)  # Fallback if detection fails\n",
        "\n",
        "    return en_words, te_words\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "test_sentence = \"This is a sample sentence with Telugu words like nenu, meeru, and bagundi.\"\n",
        "en_words, te_words = split_english_telugu(test_sentence)\n",
        "\n",
        "print(\"English words:\", en_words)\n",
        "print(\"Telugu/Other words:\", te_words)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aag7sgf0GzZo",
        "outputId": "16af1050-eb99-444b-e4d3-9750e909f0c1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Loaded dataset: (413, 1)\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.89      0.99      0.94       157\n",
            "         1.0       0.99      0.78      0.87        90\n",
            "\n",
            "    accuracy                           0.91       247\n",
            "   macro avg       0.94      0.89      0.90       247\n",
            "weighted avg       0.92      0.91      0.91       247\n",
            "\n",
            "English words: ['This', 'with', 'and']\n",
            "Telugu/Other words: ['is', 'a', 'sample', 'sentence', 'Telugu', 'words', 'like', 'nenu', 'meeru', 'bagundi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install pandas openpyxl scikit-learn langdetect\n",
        "\n",
        "# Import necessary libraries\n",
        "import re\n",
        "from langdetect import detect\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import joblib\n",
        "import os  # Import the os module for file operations\n",
        "import pandas as pd\n",
        "\n",
        "# === Step 1: Load your uploaded Excel dataset ===\n",
        "# Provide the correct path to your Excel file\n",
        "file_path = \"/content/code_mixed_sentences11.xlsx\"  # Update with your file path if necessary\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(file_path):\n",
        "    raise FileNotFoundError(f\"File not found: {file_path}. Please make sure the file is uploaded or the path is correct.\")\n",
        "\n",
        "df = pd.read_excel(file_path)  # File from your upload\n",
        "print(\"Loaded dataset:\", df.shape)\n",
        "\n",
        "# === Step 2: Language Detection Helper ===\n",
        "def detect_languages(sentence):\n",
        "    # Convert sentence to string if it's not already\n",
        "    sentence = str(sentence)  # This line is added to handle non-string values\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    lang_tags = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            lang = detect(token)\n",
        "        except:\n",
        "            lang = \"unknown\"\n",
        "        lang_tags.append(lang)\n",
        "    return tokens, lang_tags\n",
        "\n",
        "# === Step 3: Feature Extraction Function ===\n",
        "def extract_features(sentence):\n",
        "    tokens, lang_tags = detect_languages(sentence)\n",
        "    total = len(tokens)\n",
        "    en_count = lang_tags.count('en')\n",
        "    other_count = total - en_count\n",
        "    switch_count = sum(1 for i in range(1, len(lang_tags)) if lang_tags[i] != lang_tags[i-1])\n",
        "    return [total, en_count, other_count, switch_count, en_count/total if total else 0]\n",
        "\n",
        "# Apply feature extraction\n",
        "df['features'] = df['CodeMixedSentence'].apply(extract_features)\n",
        "X_cm = np.array(df['features'].tolist())\n",
        "y_cm = np.ones(len(df))  # Label 1 for code-mixed\n",
        "\n",
        "# === Step 4: Create synthetic non-code-mixed samples (label 0) ===\n",
        "synthetic_english = [\n",
        "    \"This is a nice day.\",\n",
        "    \"I am going to school.\",\n",
        "    \"We had dinner together.\",\n",
        "    \"Let's play football.\",\n",
        "    \"She reads every day.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_telugu = [\n",
        "    \"Nenu school ki veltunna.\",\n",
        "    \"Aame rojuki oka book chadutundi.\",\n",
        "    \"Manam kalisi dinner chesamu.\",\n",
        "    \"Football aadudham.\",\n",
        "    \"Idi chala bagundi.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_sentences = synthetic_english + synthetic_telugu\n",
        "y_synth = [0] * len(synthetic_sentences)\n",
        "X_synth = np.array([extract_features(sent) for sent in synthetic_sentences])\n",
        "\n",
        "# === Step 5: Combine datasets ===\n",
        "X = np.vstack([X_cm, X_synth])\n",
        "y = np.concatenate([y_cm, y_synth])\n",
        "\n",
        "# === Step 6: Train the Classifier ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# === Step 7: Evaluate the Model ===\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# === Optional: Save the model ===\n",
        "joblib.dump(clf, \"code_mixed_model.pkl\")\n",
        "\n",
        "# === Step 8: Function to split English and Telugu words ===\n",
        "def split_english_telugu(sentence):\n",
        "    \"\"\"Splits a sentence into English and Telugu/Other words.\"\"\"\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    en_words = []\n",
        "    te_words = []\n",
        "\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            lang = detect(token)\n",
        "            if lang == 'en':\n",
        "                en_words.append(token)\n",
        "            else:\n",
        "                te_words.append(token)  # Assume non-English as Telugu/Other\n",
        "        except:\n",
        "            te_words.append(token)  # Fallback if detection fails\n",
        "\n",
        "    return en_words, te_words\n",
        "\n",
        "# Example usage of the split_english_telugu function:\n",
        "test_sentence = \"This is a sample sentence with Telugu words like nenu, meeru, and bagundi.\"\n",
        "\n",
        "# Split the sentence into English and Telugu/Other words\n",
        "en_words, te_words = split_english_telugu(test_sentence)\n",
        "\n",
        "# Display the result\n",
        "print(\"\\nEnglish words:\", en_words)\n",
        "print(\"Telugu/Other words:\", te_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-woormIH-k_",
        "outputId": "7e310d20-2891-4cde-80d8-ca550e08087d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Loaded dataset: (413, 1)\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      1.00      0.94       157\n",
            "         1.0       1.00      0.77      0.87        90\n",
            "\n",
            "    accuracy                           0.91       247\n",
            "   macro avg       0.94      0.88      0.90       247\n",
            "weighted avg       0.93      0.91      0.91       247\n",
            "\n",
            "\n",
            "English words: ['This', 'with', 'and']\n",
            "Telugu/Other words: ['is', 'a', 'sample', 'sentence', 'Telugu', 'words', 'like', 'nenu', 'meeru', 'bagundi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install pandas openpyxl scikit-learn langdetect\n",
        "\n",
        "# Import necessary libraries\n",
        "import re\n",
        "from langdetect import detect\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import joblib\n",
        "import os  # Import the os module for file operations\n",
        "import pandas as pd\n",
        "\n",
        "# === Step 1: Load your uploaded Excel dataset ===\n",
        "# Provide the correct path to your Excel file\n",
        "file_path = \"/content/code_mixed_sentences11.xlsx\"  # Update with your file path if necessary\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(file_path):\n",
        "    raise FileNotFoundError(f\"File not found: {file_path}. Please make sure the file is uploaded or the path is correct.\")\n",
        "\n",
        "df = pd.read_excel(file_path)  # File from your upload\n",
        "print(\"Loaded dataset:\", df.shape)\n",
        "\n",
        "# === Step 2: Language Detection Helper ===\n",
        "def detect_languages(sentence):\n",
        "    # Convert sentence to string if it's not already\n",
        "    sentence = str(sentence)  # This line is added to handle non-string values\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    lang_tags = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            lang = detect(token)\n",
        "        except:\n",
        "            lang = \"unknown\"\n",
        "        lang_tags.append(lang)\n",
        "    return tokens, lang_tags\n",
        "\n",
        "# === Step 3: Feature Extraction Function ===\n",
        "def extract_features(sentence):\n",
        "    tokens, lang_tags = detect_languages(sentence)\n",
        "    total = len(tokens)\n",
        "    en_count = lang_tags.count('en')\n",
        "    other_count = total - en_count\n",
        "    switch_count = sum(1 for i in range(1, len(lang_tags)) if lang_tags[i] != lang_tags[i-1])\n",
        "    return [total, en_count, other_count, switch_count, en_count/total if total else 0]\n",
        "\n",
        "# Apply feature extraction\n",
        "df['features'] = df['CodeMixedSentence'].apply(extract_features)\n",
        "X_cm = np.array(df['features'].tolist())\n",
        "y_cm = np.ones(len(df))  # Label 1 for code-mixed\n",
        "\n",
        "# === Step 4: Create synthetic non-code-mixed samples (label 0) ===\n",
        "synthetic_english = [\n",
        "    \"This is a nice day.\",\n",
        "    \"I am going to school.\",\n",
        "    \"We had dinner together.\",\n",
        "    \"Let's play football.\",\n",
        "    \"She reads every day.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_telugu = [\n",
        "    \"Nenu school ki veltunna.\",\n",
        "    \"Aame rojuki oka book chadutundi.\",\n",
        "    \"Manam kalisi dinner chesamu.\",\n",
        "    \"Football aadudham.\",\n",
        "    \"Idi chala bagundi.\"\n",
        "] * (len(df) // 5)\n",
        "\n",
        "synthetic_sentences = synthetic_english + synthetic_telugu\n",
        "y_synth = [0] * len(synthetic_sentences)\n",
        "X_synth = np.array([extract_features(sent) for sent in synthetic_sentences])\n",
        "\n",
        "# === Step 5: Combine datasets ===\n",
        "X = np.vstack([X_cm, X_synth])\n",
        "y = np.concatenate([y_cm, y_synth])\n",
        "\n",
        "# === Step 6: Train the Classifier ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# === Step 7: Evaluate the Model ===\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# === Optional: Save the model ===\n",
        "joblib.dump(clf, \"code_mixed_model.pkl\")\n",
        "\n",
        "# === Step 8: Function to split English and Telugu words (with improvement) ===\n",
        "def split_english_telugu(sentence):\n",
        "    \"\"\"Splits a sentence into English and Telugu/Other words.\"\"\"\n",
        "    tokens = re.findall(r'\\w+', sentence)\n",
        "    en_words = []\n",
        "    te_words = []\n",
        "\n",
        "    # Predefined list of common English words (optional improvement)\n",
        "    common_english_words = {'hello', 'hi', 'how', 'are', 'you', 'this', 'is', 'a', 'the', 'my', 'it', 'we', 'I'}\n",
        "\n",
        "    for token in tokens:\n",
        "        # First check if it's a common English word\n",
        "        if token.lower() in common_english_words:\n",
        "            en_words.append(token)\n",
        "        else:\n",
        "            try:\n",
        "                lang = detect(token)\n",
        "                if lang == 'en':\n",
        "                    en_words.append(token)\n",
        "                else:\n",
        "                    te_words.append(token)  # Assume non-English as Telugu/Other\n",
        "            except:\n",
        "                te_words.append(token)  # Fallback if detection fails\n",
        "\n",
        "    return en_words, te_words\n",
        "\n",
        "# === Step 9: Function to test any sentence ===\n",
        "def test_code_mixed_sentence(sentence):\n",
        "    en_words, te_words = split_english_telugu(sentence)\n",
        "    print(\"English words:\", en_words)\n",
        "    print(\"Telugu/Other words:\", te_words)\n",
        "\n",
        "# Example test sentence\n",
        "test_sentence = \"hello yevaru\"\n",
        "\n",
        "# Test the sentence\n",
        "test_code_mixed_sentence(test_sentence)\n",
        "\n",
        "# Now you can enter your custom sentence:\n",
        "custom_sentence = input(\"\\nEnter a sentence with mixed English and Telugu words: \")\n",
        "test_code_mixed_sentence(custom_sentence)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u1hWpa6IpOo",
        "outputId": "b38a9854-3c7b-4d19-a237-2faabce74313"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Loaded dataset: (413, 1)\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      1.00      0.93       157\n",
            "         1.0       1.00      0.76      0.86        90\n",
            "\n",
            "    accuracy                           0.91       247\n",
            "   macro avg       0.94      0.88      0.90       247\n",
            "weighted avg       0.92      0.91      0.91       247\n",
            "\n",
            "English words: ['hello']\n",
            "Telugu/Other words: ['yevaru']\n",
            "\n",
            "Enter a sentence with mixed English and Telugu words: hi how are you nenu bagunna nuvu fine thank you\n",
            "English words: ['hi', 'how', 'are', 'you', 'thank', 'you']\n",
            "Telugu/Other words: ['nenu', 'bagunna', 'nuvu', 'fine']\n"
          ]
        }
      ]
    }
  ]
}